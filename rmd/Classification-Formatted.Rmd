---
title: "Wine Type Classification"
output: html_document
---

## 1. Data Preprocessing

Below are the head rows of the wine quality data set. The variable __quality__ here is treated as numerical for more accurate prediction.

```{r, include = FALSE, echo=FALSE}
library(tidyverse)
library(splitstackshape)
library(caret)
library(MASS)
library(pROC)
library(rpart)
library(randomForest)
library(knitr)
library(tree)
library(DT)
```
```{r, echo=FALSE}
data = read.csv("winequality_all.csv")
data %>% head()
data_red = data %>% filter(red == 1)
data_white = data %>% filter(red == 0)
print(dim(data_red))
print(dim(data_white))
```

The dimension of red wine data set is (1599, 13), and the dimension of white wine data set is (4898, 13). The white wine data set is nearly three times that of the red wine data set -- the data set is imbalanced.

## 2. Distribution of Potential Predictors

This part contains some exploratory data analysis on the wine quality data set, mainly for the purpose of classifying read wine and white wine. First we would like to explore the distribution of all the potential predictors separately for red wine and white wine, to see if there is any difference between predictor distribution in two groups.

```{r, echo=FALSE}
data %>% ggplot(aes(fixed.acidity, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Fixed Acidity") + 
  xlab("Fixed Acidity") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))

data %>% ggplot(aes(volatile.acidity, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Volatile Acidity") +
  xlab("Volatile Acidity") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(citric.acid, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Citric Acid") +
  xlab("Citric Acid") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(residual.sugar, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Residual Sugar") +
  xlab("Residual Sugar") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(chlorides, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Chlorides") +
  xlab("Chlorides") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(free.sulfur.dioxide, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Free Sulfur Dioxide") +
  xlab("Free Sulfur Dioxide") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(total.sulfur.dioxide, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Total Sulfur Dioxide") +
  xlab("Total Sulfur Dioxide") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(density, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Density") +
  xlab("Density") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(pH, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("PH") +
  xlab("PH") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(sulphates, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Sulphates") +
  xlab("Sulphates") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))


data %>% ggplot(aes(alcohol, fill = as.factor(red), y = ..density..)) + 
  geom_histogram(alpha = 0.2, bins = 50, color = "black") +
  ggtitle("Alcohol") +
  xlab("Alcohol") +
  scale_fill_discrete(name = "Category", labels = c("red", "white"))

# quality
t = table(data$quality, data$red)
t[1:7] = t[1:7]/dim(data_white)[1]
t[8:14] = t[8:14]/dim(data_red)[1]
colnames(t) <- c("white", "red")
print("Table of quality (in terms of proportion): ")
print(t)
```

From these histograms, we can see that some of the features have similar distribution among two groups, while the other features have very different distributions. The predictors __residual sugar__, __pH__, __free sulfur dioxide__, __fixed acidity__, __alcohol__, __density__ and __quality__ have similar distribution, while predictors __volatile acidity__, __citric acid__, __chlorides__, and __total sulfur dioxide__ have very different distributions among two groups. Those having different distributions may have higher importance in predicting the wine category, and may have larger impact on the models we fit.

## 3. Train-Test Split

In this section, the wine data set is splitted into training set and test set for applying machine learning algorithms later on. Here we use a 70% - 30% stratification. The dimension of training set is (4548, 13), and the dimension of test set is (1949, 13).

```{r, echo=FALSE}
data <- data %>% mutate(fixed.acidity = as.numeric(fixed.acidity),
                        volatile.acidity = as.numeric(volatile.acidity),
                        citric.acid = as.numeric(citric.acid),
                        residual.sugar = as.numeric(residual.sugar),
                        chlorides = as.numeric(chlorides),
                        free.sulfur.dioxide= as.numeric(free.sulfur.dioxide),
                        total.sulfur.dioxide = as.numeric(total.sulfur.dioxide),
                        density = as.numeric(density),
                        pH = as.numeric(pH),
                        sulphates = as.numeric(sulphates),
                        alcohol = as.numeric(alcohol),
                        quality = as.factor(quality),
                        red = as.factor(red)
)

```


```{r pressure, echo=FALSE}
set.seed(123)

x <- stratified(data, "red", 0.7, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- data[-train_index,]

dim(train_set)
dim(test_set)
```

## 4. Models Using all Predictors

In this section, we build machine learning models: logistic regression, LDA, QDA, tree model and random forest, based on all the potential predictors to predict whether the wine is red or not.

#### I. logistic regression

```{r, echo=FALSE}
set.seed(1)
fit_glm <- glm(as.factor(red) ~ . , data = train_set, family = "binomial")
probs_glm <- predict(fit_glm, newdata = test_set, type = "response")
preds_glm <- ifelse(probs_glm > 0.5, 1, 0)
confusionMatrix(as.factor(preds_glm), reference =as.factor(test_set$red))
```

The logistic regression model has great performance. It has overall accuracy of 0.9959, with sensitivity and specificity 0.9980 and 0.9896, which is pretty high. This model performs very good in predicting white wine as well as in predicting red wine.  

#### II. LDA

```{r, echo=FALSE}
set.seed(1)
lda_fit <- lda(as.factor(red) ~ . , data = train_set)
lda_probs <- predict(lda_fit, newdata = test_set)$posterior[,2]
lda_preds <- ifelse(lda_probs > 0.5, 1, 0)
confusionMatrix(as.factor(lda_preds), reference =as.factor(test_set$red), positive = "1")
```

For LDA model, the overall accuracy is slightly higher than the logistic regression model, with slightly higher specificity and lower sensitivity. 

#### III. QDA

For QDA model, since in this section the number of predictors is quite high, there is a problem of rank deficiency of red wine group of our training set, which means the parameters required by QDA model cannot be estimated from the data we currently have. Therefore, the QDA model is ignored in this section, but will be applied in following section.

#### IV. Tree Model

```{r, echo=FALSE}
# Tree
fit_tree =rpart(as.factor(red) ~ . , data=train_set)  
tree_probs <- predict(fit_tree, newdata = test_set)[,2]
tree_preds <- ifelse(tree_probs > 0.5, 1, 0)
confusionMatrix(as.factor(tree_preds), reference = as.factor(test_set$red), positive = "1")
```

The tree model here has lower accuracy compared to LDA or logistic regression model, with much lower sensitivity, which means it doesn't perform very well in predicting red wine.

```{r, echo=FALSE}
fit_tree2 = tree(red ~ . , train_set)  
plot(fit_tree2, type = "uniform")
text(fit_tree2, cex = 1)
```

Here is a visualization of tree model. We can see that the tree model only employs variables of __chlorides__, __total sulfur dioxide__, __density__, __fixed acidity__, while variables __volatile acidity__, __citric acid__, __residual sugar__, __free sulfur dioxide__, __pH__, __sulphates__, __alcohol__, __quality__ are not useful in model building. 

#### V. Random Forest

```{r, echo=FALSE}
# Random Forest
fit_rf <- randomForest(as.factor(red) ~ . , data=train_set)
rf_probs <- predict(fit_rf, newdata = test_set, type="prob")[,2]
rf_preds <- ifelse(rf_probs > 0.5, 1, 0)
confusionMatrix(as.factor(rf_preds), reference = as.factor(test_set$red), positive = "1")
```

The random forest model has high overall accuracy, high sensitivity, high specificity as well.

```{r, echo=FALSE, warning=FALSE}
variable_importance <- importance(fit_rf) 
tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini))
kable(tmp)
```

This table shows that __chlorides__ and __total sulfur dioxide__ are the two most important predictors in classifying red wine and white wine.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
roc_glm <- roc(as.factor(test_set$red), probs_glm)
roc_lda <- roc(as.factor(test_set$red), lda_probs)
roc_tree <- roc(as.factor(test_set$red), tree_probs)
roc_rf <- roc(as.factor(test_set$red), rf_probs)

ggroc(list("Logistic Regression" = roc_glm, "LDA" = roc_lda, "Decision Tree" = roc_tree, "Random Forest" = roc_rf)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  xlab("Sensitivity") +
  ylab("Specificity")
```
```{r, warning=FALSE}
auc(roc_glm)
auc(roc_lda)
auc(roc_tree)
auc(roc_rf)
```

The ROC curves and AUC all indicate that these models perform very good in predicting whether a wine is red or not. The model that has the highest AUC is the random forest model.

## 5. Build Model Using Two Predictors

Previous analysis indicates that not all of the predictors seem to be useful in building our models. Therefore, in this section, we will explore the performance of building models with only two predictors -- __chlorides__ and __total sulfur dioxide__.

```{r, echo=FALSE}
data %>% ggplot(aes(chlorides, total.sulfur.dioxide, color = red)) +
  scale_y_log10() + 
  scale_x_log10() +
  xlab("Chlorides") + 
  ylab("Total Sulfur Dioxide") +
  ggtitle("Chlorides vs Total Sulfur Dioxide by wine category") +
  geom_point(alpha = 0.5) 
```

From the above scatter plot we can see that the boundary of red wine and white wine is quite clear even with only two predictors. White wine generally has higher level of __chlorides__ and higher level of __total sulfur dioxide__. Therefore, next we will try using only two predictors to predict whether the wine is red or not.

```{r, echo=FALSE}
# logistic regression
set.seed(1)
fit_glm <- glm(as.factor(red) ~ chlorides + total.sulfur.dioxide, data = train_set, family = "binomial")
probs_glm <- predict(fit_glm, newdata = test_set, type = "response")
preds_glm <- ifelse(probs_glm > 0.5, 1, 0)
confusionMatrix(as.factor(preds_glm), reference =as.factor(test_set$red))
```

The logistic regression model using only two predictors has lower accuracy (0.9513) than the logistic regression model using all the predictors. Besides, its specificity is comparatively low, which means this model performs not that well in predicting white wine.

```{r, echo=FALSE}
# LDA 
set.seed(1)
lda_fit <- lda(as.factor(red) ~ chlorides + total.sulfur.dioxide, data = train_set)
lda_probs <- predict(lda_fit, newdata = test_set)$posterior[,2]
lda_preds <- ifelse(lda_probs > 0.5, 1, 0)
confusionMatrix(as.factor(lda_preds), reference =as.factor(test_set$red), positive = "1")
```

The LDA model has overall accuracy around 0.95 as well. It has lower sensitivity and higher specificity, which means this model performs better in predicting white wine than predicting red wine. 

```{r, echo=FALSE}
# QDA
set.seed(1)
qda_fit <- qda(red ~ chlorides + total.sulfur.dioxide, data = train_set)
qda_probs <- predict(qda_fit, newdata = test_set)$posterior[,2]
qda_preds <- ifelse(qda_probs > 0.5, 1, 0)
confusionMatrix(as.factor(qda_preds), reference = as.factor(test_set$red), positive = "1")
```

With only two predictors, fitting QDA model becomes possible. This model has slightly lower overall accuracy than logistic regression and LDA model. Here the sensitivity is comparatively low and specificity is comparatively high, which shows that this model performs better in predicting white wine than predicting red wine.

```{r, echo=FALSE}
# Tree
fit_tree =rpart(as.factor(red) ~ chlorides + total.sulfur.dioxide, data=train_set)  
tree_probs <- predict(fit_tree, newdata = test_set)[,2]
tree_preds <- ifelse(tree_probs > 0.5, 1, 0)
confusionMatrix(as.factor(tree_preds), reference = as.factor(test_set$red), positive = "1")
```

The tree model has slightly higher accuracy than the previous models, and the sensitivity and specificity is more balanced, although the sensitivity is still lower than the specificity.

```{r, echo=FALSE}
# Random Forest
fit_rf <- randomForest(as.factor(red) ~ chlorides + total.sulfur.dioxide, data=train_set)
rf_probs <- predict(fit_rf, newdata = test_set, type="prob")[,2]
rf_preds <- ifelse(rf_probs > 0.5, 1, 0)
confusionMatrix(as.factor(rf_preds), reference = as.factor(test_set$red), positive = "1")
```

The random forest model has the highest accuracy among models using only two predictors. We can see that there is a decrease of overall accuracy for each model, which is expected due to decrease of large amount of information. However, the model using two predictors still has around 97% accuracy, indicating that other predictors may not have great importance for predicting the wine category.

```{r, echo=FALSE, message = FALSE}
roc_glm <- roc(as.factor(test_set$red), probs_glm)
roc_lda <- roc(as.factor(test_set$red), lda_probs)
roc_qda <- roc(as.factor(test_set$red), qda_probs)
roc_tree <- roc(as.factor(test_set$red), tree_probs)
roc_rf <- roc(as.factor(test_set$red), rf_probs)

ggroc(list("Logistic Regression" = roc_glm, "LDA" = roc_lda, "QDA" = roc_qda, "Decision Tree" = roc_tree, "Random Forest" = roc_rf)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  xlab("Sensitivity") +
  ylab("Specificity")
```
```{r}
auc(roc_glm)
auc(roc_lda)
auc(roc_qda)
auc(roc_tree)
auc(roc_rf)
```

Either using all the potential predictors or using only two most important predictors, the random forest model has the highest the AUC value. Since AUC represents the balance between sensitivity and specificity, I would choose random forest as the best model. In terms of number of predictors, the AUC of the model using only two predictors (0.9954) is not significantly smaller than the one using all the predictors (0.9997). Therefore, if collecting the value of other potential predictors is expensive, employing only two predictors __chlorides__ and __total sulfur dioxide__ is also a good choice.
