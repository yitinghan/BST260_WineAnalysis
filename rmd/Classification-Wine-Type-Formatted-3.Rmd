---
output: html_document
---

```{r, include = FALSE, echo=FALSE, results='hide'}
library(tidyverse)
library(splitstackshape)
library(caret)
library(MASS)
library(pROC)
library(rpart)
library(randomForest)
library(knitr)
library(tree)
library(DT)

data = read.csv("winequality_all.csv")
data_red = data %>% filter(red == 1)
data_white = data %>% filter(red == 0)

data <- data %>% mutate(fixed.acidity = as.numeric(fixed.acidity),
                        volatile.acidity = as.numeric(volatile.acidity),
                        citric.acid = as.numeric(citric.acid),
                        residual.sugar = as.numeric(residual.sugar),
                        chlorides = as.numeric(chlorides),
                        free.sulfur.dioxide= as.numeric(free.sulfur.dioxide),
                        total.sulfur.dioxide = as.numeric(total.sulfur.dioxide),
                        density = as.numeric(density),
                        pH = as.numeric(pH),
                        sulphates = as.numeric(sulphates),
                        alcohol = as.numeric(alcohol),
                        quality = as.factor(quality),
                        red = as.factor(red)
)

set.seed(123)

x <- stratified(data, "red", 0.7, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- data[-train_index,]
```

## Build Model Using Two Predictors

Previous analysis indicates that not all of the predictors seem to be useful in building our models. Therefore, in this section, we will explore the performance of building models with only two predictors -- __chlorides__ and __total sulfur dioxide__.

```{r, echo=FALSE}
data %>% ggplot(aes(chlorides, total.sulfur.dioxide, color = red)) +
  scale_y_log10() + 
  scale_x_log10() +
  xlab("Chlorides") + 
  ylab("Total Sulfur Dioxide") +
  ggtitle("Chlorides vs Total Sulfur Dioxide by wine category") +
  geom_point(alpha = 0.5) 
```

From the above scatter plot we can see that the boundary of red wine and white wine is quite clear even with only two predictors. White wine generally has higher level of __chlorides__ and higher level of __total sulfur dioxide__. Therefore, next we will try using only two predictors to predict whether the wine is red or not.

### logistic regression
```{r, echo=FALSE}
# logistic regression
set.seed(1)
fit_glm <- glm(as.factor(red) ~ chlorides + total.sulfur.dioxide, data = train_set, family = "binomial")
probs_glm <- predict(fit_glm, newdata = test_set, type = "response")
preds_glm <- ifelse(probs_glm > 0.5, 1, 0)
confusionMatrix(as.factor(preds_glm), reference =as.factor(test_set$red))
```

The logistic regression model using only two predictors has lower accuracy (0.9513) than the logistic regression model using all the predictors. Besides, its specificity is comparatively low, which means this model performs not that well in predicting white wine.

### LDA
```{r, echo=FALSE}
# LDA 
set.seed(1)
lda_fit <- lda(as.factor(red) ~ chlorides + total.sulfur.dioxide, data = train_set)
lda_probs <- predict(lda_fit, newdata = test_set)$posterior[,2]
lda_preds <- ifelse(lda_probs > 0.5, 1, 0)
confusionMatrix(as.factor(lda_preds), reference =as.factor(test_set$red), positive = "1")
```

The LDA model has overall accuracy around 0.95 as well. It has lower sensitivity and higher specificity, which means this model performs better in predicting white wine than predicting red wine. 

### QDA
```{r, echo=FALSE}
# QDA
set.seed(1)
qda_fit <- qda(red ~ chlorides + total.sulfur.dioxide, data = train_set)
qda_probs <- predict(qda_fit, newdata = test_set)$posterior[,2]
qda_preds <- ifelse(qda_probs > 0.5, 1, 0)
confusionMatrix(as.factor(qda_preds), reference = as.factor(test_set$red), positive = "1")
```

With only two predictors, fitting QDA model becomes possible. This model has slightly lower overall accuracy than logistic regression and LDA model. Here the sensitivity is comparatively low and specificity is comparatively high, which shows that this model performs better in predicting white wine than predicting red wine.

### Tree Model
```{r, echo=FALSE}
# Tree
fit_tree =rpart(as.factor(red) ~ chlorides + total.sulfur.dioxide, data=train_set)  
tree_probs <- predict(fit_tree, newdata = test_set)[,2]
tree_preds <- ifelse(tree_probs > 0.5, 1, 0)
confusionMatrix(as.factor(tree_preds), reference = as.factor(test_set$red), positive = "1")
```

The tree model has slightly higher accuracy than the previous models, and the sensitivity and specificity is more balanced, although the sensitivity is still lower than the specificity.

### Random Forest
```{r, echo=FALSE}
# Random Forest
fit_rf <- randomForest(as.factor(red) ~ chlorides + total.sulfur.dioxide, data=train_set)
rf_probs <- predict(fit_rf, newdata = test_set, type="prob")[,2]
rf_preds <- ifelse(rf_probs > 0.5, 1, 0)
confusionMatrix(as.factor(rf_preds), reference = as.factor(test_set$red), positive = "1")
```

The random forest model has the highest accuracy among models using only two predictors. We can see that there is a decrease of overall accuracy for each model, which is expected due to decrease of large amount of information. However, the model using two predictors still has around 97% accuracy, indicating that other predictors may not have great importance for predicting the wine category.

```{r, echo=FALSE, message = FALSE}
roc_glm <- roc(as.factor(test_set$red), probs_glm)
roc_lda <- roc(as.factor(test_set$red), lda_probs)
roc_qda <- roc(as.factor(test_set$red), qda_probs)
roc_tree <- roc(as.factor(test_set$red), tree_probs)
roc_rf <- roc(as.factor(test_set$red), rf_probs)

ggroc(list("Logistic Regression" = roc_glm, "LDA" = roc_lda, "QDA" = roc_qda, "Decision Tree" = roc_tree, "Random Forest" = roc_rf)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  xlab("Sensitivity") +
  ylab("Specificity")
```
```{r, echo = FALSE, message=FALSE}
print(paste0("For logistic regression model, the AUC is ", auc(roc_glm)))
print(paste0("For LDA model, the AUC is ", auc(roc_lda)))
print(paste0("For QDA model, the AUC is ", auc(roc_qda)))
print(paste0("For tree model, the AUC is ", auc(roc_tree)))
print(paste0("For random forest model, the AUC is ", auc(roc_rf)))
```

Either using all the potential predictors or using only two most important predictors, the random forest model has the highest the AUC value. Since AUC represents the balance between sensitivity and specificity, I would choose random forest as the best model. In terms of number of predictors, the AUC of the model using only two predictors (0.9954) is not significantly smaller than the one using all the predictors (0.9997). Therefore, if collecting the value of other potential predictors is expensive, employing only two predictors __chlorides__ and __total sulfur dioxide__ is also a good choice.
